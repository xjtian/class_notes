\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[retainorgcmds]{IEEEtrantools}

\usepackage[pdftex]{graphicx}
\usepackage{tikz}
\usetikzlibrary{intersections}

\usepackage{marginnote}
\usepackage{endnotes}

\usepackage{fancyhdr}

%Listings stuff
\usepackage{listings}
\usepackage{lstautogobble}
\usepackage{color}

\definecolor{gray}{rgb}{0.5,0.5,0.5}
\lstset{
basicstyle={\small\ttfamily},
tabsize=3,
numbers=left,
numbersep=5pt,
numberstyle=\tiny\color{gray},
stepnumber=2,
breaklines=true
}

%Properly formatted differential 'd'
\newcommand{\ud}{\, \mathrm{d}}

%Format stuff
\pagestyle{fancy}
\headheight 35pt

%Header info
\chead{\Large \textbf{Vector Differential Calculus}}
\lhead{}
\rhead{}

\begin{document}
\section{Gradient Fields}
	If $f$ is a differential real-valued function, $\vec{\nabla} f$ can be thought of as a \textbf{vector field}. If $\vec{\nabla} f = \vec{F}(x, y) = (f(x, y), g(x, y))$, then to draw the gradient field, at each point $(x, y)$ in the domain is an arrow from $(0,0)$ to $(\vec{F}(x, y)$. The arrow is then translated to start at $(x, y)$ and end at $\vec{F}(x, y) + (x, y)$.
	
	From the gradient field of any differentiable function $f$, it becomes apparent that at each point in the domain for which $\vec{\nabla} f \neq \vec{0}$, $\vec{nabla} f$ points in the direction of maximum increase. The number $|\vec{\nabla} f|$ is the maximum rate of increase of $f$ at $\vec{x}$.
	
	\subparagraph{Normal Vectors} If $\mathbb{R}^n \xrightarrow{f} \mathbb{R}$ is continuously differentiable at $\vec{x}_0$ and $S$ is a level set containing $\vec{x}_0$, then $\vec{\nabla}f$ is a normal vector to $S$ at $\vec{x}_0$ and
	\begin{equation}
		\vec{\nabla}f(\vec{x}_0) \cdot (\vec{x}-\vec{x}_0) = 0
	\end{equation}
	
\section{Chain Rule}
	\begin{equation}
		\frac{dg}{dx} (f(x)) = g'(f(x))f'(x)
	\end{equation}
	
	In a special case, if $\mathbb{R}^n \xrightarrow{f} \mathbb{R}^n \xrightarrow{g} \mathbb{R}^p$ and $f(\vec{x}) = A\vec{x}$ and $g(\vec{x}) = B\vec{x}$, because the derivative matrix of $f$ and $g$ are just $A$ and $B$, $(g\circ f)'= g'f'$.
	
	If $f$ and $g$ are continuously differentiable near $\vec{x}$ and $g\circ f$ is defined on an open set containing $\vec{x}$, then $g\circ f$ is continuously differentiable at $\vec{x}$.
	
	\subparagraph{Changing Variables} Changing the variables of a function makes it $f(g(x), h(y))$, which can be differentiated with the chain rule. The Laplace operator $\Delta$ is defined as follows only if there has been a change of variables: 
	\[\Delta u(x, y) = u_{xx}(x, y) + u_{yy}(x y)\]
	When only transforming coordinates, especially if the domain and range are the same, use the term \textbf{coordinate transformation}, usually accomplished with $\vec{z} = A\vec{x}$ where $\vec{z}$ is the vector of the variables to change to.
	
	\subparagraph{Inverse Function Theorem} If $\mathbb{R}^n \xrightarrow{F} \mathbb{R}^n$ is continuously differentiable and $\vec{x}_0$ is a point with an invertible derivative matrix $F'(\vec{x}_0)$, then there is an open neighborhood $N$ of $\vec{x}_0$ such that $F$ has a continuously differentiable inverse function $F^{-1}$ defined on the image set $F(N)$. The derviative matrix of $F^{-1}$ is related to $F'(\vec{x})$ by $(F^{-1})'(F(\vec{x})) = F'(\vec{x}) ^ {-1}$ for $\vec{x}$ in $N$.
	
	In other words, there is a continuously differentiable \textbf{local inverse} on some neighborhood of a point where $F'(\vec{x}_0)$ is invertible,. $det F'(\vec{x})$ is called the \textbf{Jacobian determinant}.
	
\section{Implicit Differentiation}
	Given $\mathbb{R}^2 \xrightarrow{F} \mathbb{R}$ and $\mathbb{R} \xrightarrow{f} \mathbb{R}$, $F(x, y)$ defines $f$ implicitly if $F(x, f(x)) = c$ for every $x$ in the domain.
	\begin{IEEEeqnarray}{rCl}
		F(x, G(x)) & = & 0\\
		F_x(x, G(x)) + F_y(x, y)G'(x) & = & 0\\
		F_x(x, y) + F_y(x, y)\frac{dy}{dx} & = & 0\\
		\frac{dy}{dx} = -\frac{F_x(x, y)}{F_y(x, y)}
	\end{IEEEeqnarray}
	
	When implicitly differentiating vector-valued functions, decide which variable to differentiate with respect to, then use the chain rule and solve for the differential terms to get a solution matrix that corresponds to $G'(x_i)$, where $x_i$ is the variable that the original derivative was taken in. When there multiple free (so to say) variables to account for (i.e., $x, y, z$ are functions of $u, v, w$), take the partial with respect to one of the free ones.
	\subparagraph{Theorem} suppose $\mathbb{R}^{n+m} \xrightarrow{F} \mathbb{R}^m$ and $\mathbb{R}^n \xrightarrow{G} \mathbb{R}^m$ are differentiable and $F(\vec{x}, \vec{y})$ implicitly defines $\vec{y} = G(\vec{x})$, then
	\[G'(\vec{x} = -F_y^{-1} (\vec{x}, G(\vec{x}))F_x(\vec{x}, G(\vec{x}))\]
	provided that $F_y$ is invertible. $F_y$ is computed with $\vec{x}$ held fixed and $F_x$ is computed with $\vec{y}$ held fixed.
	
	\subparagraph{Implicit Function Theorem} Let $\mathbb{R}^{n+m} \xrightarrow{F} \mathbb{R}^m$ be continuously differentiable. Suppose for some $\vec{x}_0 \in \mathbb{R}^n$ and some $\vec{y}_0 \in \mathbb{R}^m$ that
	\begin{itemize}
		\item $F(\vec{x}_0, \vec{y}_0) = \vec{0}$
		\item $F_y(\vec{x}_0, \vec{y}_0)$ is an invertible $m$-by-$n$ matrix.
	\end{itemize}
	Then there is a unique continuously differentiable function $\mathbb{R}^n \xrightarrow{G} \mathbb{R}^m$ defined on an open neighborhood $N$ of $\vec{x}_0$ in $\mathbb{R}^n$ such that $F$ implicitly defines $G$ and $G(\vec{x}_0) = \vec{y}_0$.

\section{Extreme Values}
	Where $\vec{\nabla} f(\vec{x}) = \vec{0}$, $f$ has a local extrema. Note that A point where $\vec{\nabla} f = \vec{0}$ is not necessarily an extreme point $f$ may have an extreme point without having $\vec{\nabla} f = \vec{0}$. The second condition is why it's important to check the values at the bounds of the locale.
	
	\subparagraph{Lagrange Multipliers} Given $\mathbb{R}^n \xrightarrow{f} \mathbb{R}$ is differentiable and there is a local extreme at $\vec{x}_0$, and that near $\vec{x}_0$, the domain of $f$ is restricted to a level set of $\mathbb{R}^n \xrightarrow{G} \mathbb{R}^m$ and coordinate functions $G_1, G_2, \ldots , G_m$. Then the following holds true for some $\vec{\lambda}$:
	\begin{equation}
		\vec{\nabla} f + \lambda_1 \vec{\nabla} G_1(\vec{x}_0) + \ldots + \lambda_m \vec{\nabla} G_m(\vec{x}_0) = \vec{0}
	\end{equation}
	This method is a necessary condition for extrema, but may also hold true for points that are not extrema. It is also necessary to have some grounds for believing that the desired extrema exist, and that the set $S$ to which $f$ is restricted is closed, bounded, and sufficiently smooth.
	
	\subparagraph{Saddle Points} A critical point that is neither a local maximum nor a local minimum is called a \textbf{saddle point} for $f$.
	
	\subsection{Second Derivative Test}
		Given $\mathbb{R}^n \xrightarrow{f} \mathbb{R}$ is twice continuously differentiable and $c = \dfrac{\partial^2 f}{\partial \vec{u}^2} (\vec{x}_0)$:
		\begin{itemize}
			\item If $c > 0$ for all unit vectors $\vec{u}$, then the point is a strict local minimum.
			\item If $c < 0$ for all unit vectors $\vec{u}$, then the point is a strict local maximum.
			\item If $c$ is positive for some $\vec{u}$ and negative for others, then the point is a saddle point.
			\item If $c = 0$ for all $\vec{u}$, the the test yields no information.
		\end{itemize}
		
		A expression for the second derivative is as follows:
		\begin{equation}
			\frac{\partial^2 f}{\partial \vec{u}^2}(\vec{x}_0) = f_{xx}(\vec{x}_0)u^2 + 2f_{xy}(\vec{x}_0)uv + f_{yy}(\vec{x}_0)v^2
		\end{equation}
		
	\subparagraph{Alternate Method} Let $D = f_{xx}(x_0, y_0)f_{yy}(x_0, y_0) - f_{xy}^2 (x_0, y_0)$:
		\begin{itemize}
			\item If $D > 0$ and $f_{xx} > 0$ or $f_{yy} > 0$ then the point is a strict minimum.
			\item If $D > 0$ and $f_{xx} < 0$ or $f_{yy} < 0$ then the point is a strict maximum.
			\item If $D = 0$ then the point is a saddle point.
		\end{itemize}
		
	\subsection{Steepest Ascent/Descent Method}
		Remember that $\vec{\nabla} f$ points in the direction of maximum increase and $-\vec{\nabla} f$ points in the direction of maximum decrease for a level set of a function $f$. Then to find the local maximum (ascent) or minimum (descent) of $f$, start at point and ``march'' in the direction of the gradient.
		\begin{equation}
			\vec{x}_{n+1} = \vec{x}_n + h_n \vec{\nabla} f(\vec{x}_n)
		\end{equation}
		Where $h_0 > 0$ for ascent and $h_n < 0$ for descent - choose an arbitrary value for the step. As long as the initial value of $h$ isn't too large, it can remain the same because $\vec{\nabla} f$ approaches 0 as we approach a local extrema.
		
		This method is the same as finding approximate solutions to the system
		\begin{equation}
			\frac{dx}{dt} = f_x(x, y), \quad \frac{dy}{dt} = f_y(x, y)
		\end{equation}
		
		
%	\begin{center}
%	\begin{tikzpicture}
%		[scale=3,line cap=round,
%		%Styles
%		axes/.style=,
%		important line/.style={very thick},
%		information text/.style={rounded corners,fill=red!10,inner sep=1ex},
%		dot/.style={circle,inner sep=1pt,fill,label={#1},name=#1}			
%		]
%		
%		%Colors
%		\colorlet{anglecolor}{green!50!black}	%angle arcs/lines
%		
%		%The graphic
%	\end{tikzpicture}
%	\end{center}

%	\begin{figure}[htb]
%		\centering
%		\includegraphics[width=0.8\textwidth]{filename.eps}
%		\caption{Caption.}
%		\label{fig:figure}
%	\end{figure}

%		\def\enotesize{\normalsize}
%		\theendnotes
\end{document}