\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[retainorgcmds]{IEEEtrantools}

\usepackage[pdftex]{graphicx}
\usepackage{tikz}
\usetikzlibrary{intersections}

\usepackage{marginnote}
\usepackage{endnotes}

\usepackage{fancyhdr}

%Listings stuff
\usepackage{listings}
\usepackage{lstautogobble}
\usepackage{color}

\definecolor{gray}{rgb}{0.5,0.5,0.5}
\lstset{
basicstyle={\small\ttfamily},
tabsize=3,
numbers=left,
numbersep=5pt,
numberstyle=\tiny\color{gray},
stepnumber=2,
breaklines=true
}

%Properly formatted differential 'd'
\newcommand{\ud}{\, \mathrm{d}}

%Format stuff
\pagestyle{fancy}
\headheight 35pt

%Header info
\chead{\Large \textbf{Vector Differential Calculus}}
\lhead{}
\rhead{}

\begin{document}
\section{Gradient Fields}
	If $f$ is a differential real-valued function, $\vec{\nabla} f$ can be thought of as a \textbf{vector field}. If $\vec{\nabla} f = \vec{F}(x, y) = (f(x, y), g(x, y))$, then to draw the gradient field, at each point $(x, y)$ in the domain is an arrow from $(0,0)$ to $\vec{F}(x, y)$. The arrow is then translated to start at $(x, y)$ and end at $\vec{F}(x, y) + (x, y)$.
	
	\marginnote{Direction of $\nabla$?}From the gradient field of any differentiable function $f$, it becomes apparent that at each point in the domain for which $\vec{\nabla} f \neq \vec{0}$, $\vec{\nabla} f$ points in the direction of maximum increase. The number $|\vec{\nabla} f|$ is the maximum rate of increase of $f$ at $\vec{x}$.
	
	\subparagraph{Normal Vectors} If \marginnote{Normal vector?}$\mathbb{R}^n \xrightarrow{f} \mathbb{R}$ is continuously differentiable at $\vec{x}_0$ and $S$ is a level set containing $\vec{x}_0$, then $\vec{\nabla}f$ is a normal vector to $S$ at $\vec{x}_0$ and
	\begin{equation}
		\vec{\nabla}f(\vec{x}_0) \cdot (\vec{x}-\vec{x}_0) = 0
	\end{equation}
	
\section{Chain Rule}
	\begin{equation}
		\frac{dg}{dx} (f(x)) = g'(f(x))f'(x)
	\end{equation}
	
	In a special case, if $\mathbb{R}^n \xrightarrow{f} \mathbb{R}^n \xrightarrow{g} \mathbb{R}^p$ and $f(\vec{x}) = A\vec{x}$ and $g(\vec{x}) = B\vec{x}$, because the derivative matrix of $f$ and $g$ are just $A$ and $B$, $(g\circ f)'= g'f'$.
	
	If $f$ and $g$ are continuously differentiable near $\vec{x}$ and $g\circ f$ is defined on an open set containing $\vec{x}$, then $g\circ f$ is continuously differentiable at $\vec{x}$.
	
	\subparagraph{Changing Variables} Changing the variables of a function makes it $f(g(x), h(y))$, which can be differentiated with the chain rule. The Laplace operator $\Delta$ is defined as follows only if there has been a change of variables: 
	\[\Delta u(x, y) = u_{xx}(x, y) + u_{yy}(x, y)\]
	When only transforming coordinates, especially if the domain and range are the same, use the term \textbf{coordinate transformation}, usually accomplished with $\vec{z} = A\vec{x}$ where $\vec{z}$ is the vector of the variables to change to.
	
	\subparagraph{Inverse Function Theorem} If \marginnote{Inverse function theorem?} $\mathbb{R}^n \xrightarrow{F} \mathbb{R}^n$ is continuously differentiable and $\vec{x}_0$ is a point with an invertible derivative matrix $F'(\vec{x}_0)$, then there is an open neighborhood such that $F$ has a continuously differentiable local inverse function $F^{-1}$.
	\begin{equation}	
		(F^{-1})'(F(\vec{x})) = F'(\vec{x}) ^ {-1}
	\end{equation}
	
	In other words, there is a continuously differentiable \textbf{local inverse} on some neighborhood of a point where $F'(\vec{x}_0)$ is invertible. $det F'(\vec{x})$ is called the \textbf{Jacobian determinant}.
	
\section{Implicit Differentiation}
	\marginnote{Implicit definition?}Given $\mathbb{R}^2 \xrightarrow{F} \mathbb{R}$ and $\mathbb{R} \xrightarrow{f} \mathbb{R}$, $F(x, y)$ defines $f$ implicitly if $F(x, f(x)) = c$ for every $x$ in the domain.
	\begin{IEEEeqnarray}{rCl}
		F(x, G(x)) & = & 0\\
		F_x(x, G(x)) + F_y(x, y)G'(x) & = & 0\\
		F_x(x, y) + F_y(x, y)\frac{dy}{dx} & = & 0\\
		\frac{dy}{dx} = -\frac{F_x(x, y)}{F_y(x, y)}
	\end{IEEEeqnarray}
	
	\marginnote{Implicit diff. of vector-valued functions?}When implicitly differentiating vector-valued functions, decide which variable to differentiate with respect to, then use the chain rule and solve for the differential terms to get a solution matrix that corresponds to $G'(x_i)$, where $x_i$ is the variable that the original derivative was taken in. When there multiple free (so to say) variables to account for (i.e., $x, y, z$ are functions of $u, v, w$), take the partial with respect to one of the free ones.
	\subparagraph{Theorem} suppose $\mathbb{R}^{n+m} \xrightarrow{F} \mathbb{R}^m$ and $\mathbb{R}^n \xrightarrow{G} \mathbb{R}^m$ are differentiable and $F(\vec{x}, \vec{y})$ implicitly defines $\vec{y} = G(\vec{x})$, then
	\[G'(\vec{x}) = -F_y^{-1} (\vec{x}, G(\vec{x}))F_x(\vec{x}, G(\vec{x}))\]
	provided that $F_y$ is invertible. $F_y$ is computed with $\vec{x}$ held fixed and $F_x$ is computed with $\vec{y}$ held fixed.
	
	\subparagraph{Implicit Function Theorem} Let $\mathbb{R}^{n+m} \xrightarrow{F} \mathbb{R}^m$ be continuously differentiable. Suppose for some $\vec{x}_0 \in \mathbb{R}^n$ and some $\vec{y}_0 \in \mathbb{R}^m$ that
	\begin{itemize}
		\item $F(\vec{x}_0, \vec{y}_0) = \vec{0}$
		\item $F_y(\vec{x}_0, \vec{y}_0)$ is an invertible $m$-by-$n$ matrix.
	\end{itemize}
	Then there is a unique continuously differentiable function $\mathbb{R}^n \xrightarrow{G} \mathbb{R}^m$ defined on an open neighborhood $N$ of $\vec{x}_0$ in $\mathbb{R}^n$ such that $F$ implicitly defines $G$ and $G(\vec{x}_0) = \vec{y}_0$.

\section{Extreme Values}
	\marginnote{Location of critical points?}Where $\vec{\nabla} f(\vec{x}) = \vec{0}$, $f$ has a critical point. Note that a point where $\vec{\nabla} f = \vec{0}$ is not necessarily an extreme point: $f$ may have an extreme point without having $\vec{\nabla} f = \vec{0}$. The second condition is why it's important to check the values at the bounds of the locale.
	
	\subparagraph{Lagrange Multipliers} Given \marginnote{Lagrange method?}$\mathbb{R}^n \xrightarrow{f} \mathbb{R}$ is differentiable and there is a local extreme at $\vec{x}_0$, and that near $\vec{x}_0$, the domain of $f$ is restricted to a level set of $\mathbb{R}^n \xrightarrow{G} \mathbb{R}^m$ and coordinate functions $G_1, G_2, \ldots , G_m$. Then the following holds true for some $\vec{\lambda}$:
	\begin{equation}
		\vec{\nabla} f + \lambda_1 \vec{\nabla} G_1(\vec{x}_0) + \ldots + \lambda_m \vec{\nabla} G_m(\vec{x}_0) = \vec{0}
	\end{equation}
	This method is a necessary condition for extrema, but may also hold true for points that are not extrema. It is also necessary to have some grounds for believing that the desired extrema exist, and that the set $S$ to which $f$ is restricted is closed, bounded, and sufficiently smooth.
	
	\subparagraph{Saddle Points} A \marginnote{Saddle point?}critical point that is neither a local maximum nor a local minimum is called a \textbf{saddle point} for $f$.
	
	\subsection{Second Derivative Test}
		\marginnote{Second derivative test?}Given $\mathbb{R}^n \xrightarrow{f} \mathbb{R}$ is twice continuously differentiable and $c = \dfrac{\partial^2 f}{\partial \vec{u}^2} (\vec{x}_0)$:
		\begin{itemize}
			\item If $c > 0$ for all unit vectors $\vec{u}$, then the point is a strict local minimum.
			\item If $c < 0$ for all unit vectors $\vec{u}$, then the point is a strict local maximum.
			\item If $c$ is positive for some $\vec{u}$ and negative for others, then the point is a saddle point.
			\item If $c = 0$ for all $\vec{u}$, the the test yields no information.
		\end{itemize}
		
		A expression for the second derivative is as follows:
		\begin{equation}
			\frac{\partial^2 f}{\partial \vec{u}^2}(\vec{x}_0) = f_{xx}(\vec{x}_0)u^2 + 2f_{xy}(\vec{x}_0)uv + f_{yy}(\vec{x}_0)v^2
		\end{equation}
		
	\subparagraph{Alternate Method} Let $D = f_{xx}(x_0, y_0)f_{yy}(x_0, y_0) - f_{xy}^2 (x_0, y_0)$:
		\begin{itemize}
			\item If $D > 0$ and $f_{xx} > 0$ or $f_{yy} > 0$ then the point is a strict minimum.
			\item If $D > 0$ and $f_{xx} < 0$ or $f_{yy} < 0$ then the point is a strict maximum.
			\item If $D = 0$ then the point is a saddle point.
		\end{itemize}
		
	\subsection{Steepest Ascent/Descent Method}
		\marginnote{Steepest ascent algorithm?}Remember that $\vec{\nabla} f$ points in the direction of maximum increase and $-\vec{\nabla} f$ points in the direction of maximum decrease for a level set of a function $f$. Then to find the local maximum (ascent) or minimum (descent) of $f$, start at point and ``march'' in the direction of the gradient.
		\begin{equation}
			\vec{x}_{n+1} = \vec{x}_n + h_n \vec{\nabla} f(\vec{x}_n)
		\end{equation}
		Where $h_0 > 0$ for ascent and $h_n < 0$ for descent - choose an arbitrary value for the step. As long as the initial value of $h$ isn't too large, it can remain the same because $\vec{\nabla} f$ approaches 0 as we approach a local extrema.
		
		This method is the same as finding approximate solutions to the system
		\begin{equation}
			\frac{dx}{dt} = f_x(x, y), \quad \frac{dy}{dt} = f_y(x, y)
		\end{equation}
		
\section{Curvilinear Coordinates}
	\subparagraph{Polar Coordinates}
		\begin{equation}
			\begin{pmatrix}
				x\\y
			\end{pmatrix}
			= P \begin{pmatrix}
				r\\\theta
			\end{pmatrix}
			= \begin{pmatrix}
				r\cos \theta\\
				r\sin \theta
			\end{pmatrix}, \quad
			\left\lbrace
			\begin{matrix}
				0 < r < \infty\\
				0 \leq \theta < 2\pi
			\end{matrix}
			\right.
		\end{equation}
		\marginnote{Polar coordinates?}This mapping is not one-to-one unless $\theta$ is restricted to a subset defined by $\theta_0 \leq \theta < \theta_0 + 2\pi$. Note that when mapping the origin of the $xy$-plane, there is no appropriate polar coordinate because the function is not one-to-one. 
		
	\subparagraph{Spherical Coordinates}
		\begin{equation}
			S \begin{pmatrix}
				r\\\phi\\\theta
			\end{pmatrix}
			= \begin{pmatrix}
				r\sin\phi\cos\theta\\
				r\sin\phi\sin\theta\\
				r\cos\phi
			\end{pmatrix}, \quad
			\left\lbrace
			\begin{matrix}
				0 < r < \infty\\
				0 \leq \phi < \pi\\
				0 \leq \theta < 2\pi
			\end{matrix}
			\right.
		\end{equation}
		\marginnote{Spherical coordinates?}Spherical coordinates a rotation of $\theta$ around the equator counter-clockwise followed by a rotation of $\phi$ up along the meridian with the positive direction towards the north pole.
		
	\subparagraph{Cylindrical Coordinates} These \marginnote{Cylindrical coordinates?}are the 3-dimensional analog of polar coordinates.
		\begin{equation}
			\begin{pmatrix}
				x\\y\\z
			\end{pmatrix}
			= \begin{pmatrix}
				r\cos\theta\\
				r\sin\theta\\
				z
			\end{pmatrix}, \quad
			\left\lbrace
			\begin{matrix}
				0 < r < \infty\\
				-\pi < \theta \leq \pi\\
				-\infty < z < \infty
			\end{matrix}
			\right.
		\end{equation}
		
	\subsection{Jacobian Matrices}
		Generalizing from the three preceding examples, curvilinear coordinates in $\mathbb{R}^n$ are determined by $\mathbb{U}^n \xrightarrow{T} \mathbb{R}^n$, given that on some restriction on the domain, the map is one-to-one and $T$ has an inverse. then the curvilinear coordinates of a point $\vec{x}$ are
		\begin{equation}
			\begin{pmatrix}
				u_1\\
				\vdots\\
				u_n
			\end{pmatrix}
			= T^{-1} \begin{pmatrix}
				x_1\\
				\vdots\\
				x_n
			\end{pmatrix}
		\end{equation}
		
		\marginnote{Jacobian matrix?}The derivative matrix of any differentiable coordinate transformation is called a \textbf{Jacobian matrix}. Each column of a Jacobian matrix is the $j\textsuperscript{th}$ partial of $T$, also the tangent vector to the curvilinear coordinate curve formed by only varying the $j\textsuperscript{th}$ coordinate. 
		
\section*{Important Concepts}
	\begin{itemize}
		\item $\vec{\nabla}f$ is the vector showing the maximum rate of increase.
		\item $\vec{\nabla}f$ is normal to the level set.
		\item If $\mathbb{R}^n \xrightarrow{F} \mathbb{R}^n$ and $F'$ is invertible, then $(F')^{-1} = (F^{-1})'F$
		\item Use Lagrange to find extrema when the graph is constrained near the critical point by solving $\vec{\nabla} f + \lambda_1 \vec{\nabla} G_1(\vec{x}_0) + \ldots + \lambda_m \vec{\nabla} G_m(\vec{x}_0) = \vec{0}$.
		\item Second derivative test is the same as one-variable, except take the directional derivative twice and see if condition holds for all unit vectors.
		\item Saddle point exists if the second derivative is sometimes positive, sometimes negative.
		\item The derivative matrix of a coordinate transformation is the Jacobian matrix.
	\end{itemize}
		
%	\begin{center}
%	\begin{tikzpicture}
%		[scale=3,line cap=round,
%		%Styles
%		axes/.style=,
%		important line/.style={very thick},
%		information text/.style={rounded corners,fill=red!10,inner sep=1ex},
%		dot/.style={circle,inner sep=1pt,fill,label={#1},name=#1}			
%		]
%		
%		%Colors
%		\colorlet{anglecolor}{green!50!black}	%angle arcs/lines
%		
%		%The graphic
%	\end{tikzpicture}
%	\end{center}

%	\begin{figure}[htb]
%		\centering
%		\includegraphics[width=0.8\textwidth]{filename.eps}
%		\caption{Caption.}
%		\label{fig:figure}
%	\end{figure}

%		\def\enotesize{\normalsize}
%		\theendnotes
\end{document}