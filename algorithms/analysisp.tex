\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[retainorgcmds]{IEEEtrantools}

\usepackage{marginnote}
\usepackage{endnotes}

\usepackage{fancyhdr}

%Listings stuff
\usepackage{listings}
\usepackage{lstautogobble}
\usepackage{color}

\definecolor{gray}{rgb}{0.5,0.5,0.5}
\lstset{
basicstyle={\small\ttfamily},
tabsize=3,
numbers=left,
numbersep=5pt,
numberstyle=\tiny\color{gray},
stepnumber=2,
breaklines=true
}

%Properly formatted differential 'd'
\newcommand{\ud}{\, \mathrm{d}}

%Format stuff
\pagestyle{fancy}
\headheight 35pt

%Header info
\chead{\Large \textbf{Analysis of Algorithms}}
\lhead{}
\rhead{}

\begin{document}
\section{Mathematical Models}
	Every basic operation in any programming language has a constant running time associated with it. Therefore, a very na\"{i}ve way to determine the cost/running time of an algorithm is to sum the number of basic operations.
	
	There are a number of different simplifications that can be applied to this process of analyzation.
	\begin{description}
		\item[Cost model:]\marginnote{Cost and tilde models?} Use the most expensive basic operation as the proxy for running time.
		\item[Tilde notation:] Estimate running time or memory as a function of input size $N$, and ignore lower order terms. E.g., $1/6 N^3 - 1/2 N^2 + 1/3 N \approx 1/6 N^3$.
	\end{description}
	
	In principle, accurate mathematical models can be computed for every algorithm, but for the sake of analysis, it is often simpler to approximate the running time for large values of $N$.
	
\section{Order-of-Growth Classifications}
	\marginnote{All order-of-growth classes?}The small set of functions $1, \log N, N, N\log N, N^2, N^3, \text{and } 2^N$ are generally sufficient to describe order-of-growth of typical algorithms. In the end, a \textit{linear or linearithmic} algorithm is needed to keep pace with Moore's law.
	
\section{Theory of Algorithms}
	There are three types of analyses for the running time of algorithms.
	\begin{enumerate}
		\item \marginnote{Three cases for algorithm analysis?} Best case: lower bound on cost, determined by "easiest" input. This provides a goal for all inputs in the domain.
		\item Worst case: upper bound on cost, determined by "most difficult" input. This provides a guarantee for all inputs in the domain.
		\item Average case: expected cost for random input that provides a way to predict performance.
	\end{enumerate}
	
	These three types of analyses lead to two different approaches for algorithm design: to design for the worst case, and to randomize an algorithm and depend on probabilistic guarantees of the domain.
	
	\begin{center}
	\begin{tabular}[t]{ccc}
	Notation	&	Provides	&	Used To\\\hline
	\\Tilde		&	Leading term	&	Provide approximate model\\\\
	Big Theta	&	Asymptotic order of growth	&	Classify algorithms\\\\
	Big Oh		&	$\Theta (N^2)$ and smaller	&	Develop upper bounds\\\\
	Big Omega	&	$\Theta (N^2)$ and larger	&	Develop lower bounds
	\end{tabular}
	\end{center}
	
\section{Memory}
	\marginnote{How much memory for pointers?}Approximate the memory usage of an algorithm to determine if it will run on a given machine. Due to modern advancements, it is safe to assume that all pointers are 8 bytes (64-bit processors), which means that pointers can address more memory but also take up more memory.
	
	\marginnote{How much memory for object overhead?}
	\begin{center}
	\begin{tabular}[t]{l l}
	\textbf{Type}	&	\textbf{Bytes}	\\\hline
	\\\verb|char[]|	&	$2N + 24$\\
	\verb|int[]|	&	$4N + 24$\\
	\verb|double[]|	&	$8N + 24$\\\\\hline
	\\\verb|char[][]|	&	$\approx 2MN$\\
	\verb|int[][]|	&	$\approx 4MN$\\
	\verb|double[][]|	&	$\approx 8MN$\\\\\hline
	\\Object overhead	&	16\\
	Reference		&	8\\
	Padding			&	Up to multiple of 8
	\end{tabular}
	\end{center}
	
	\endnotetext[1]{Best to analyze algorithms based on a combination of cost and tilde models.}
	\endnotetext[2]{16 bytes of memory for object overhead, 8 bytes for reference, 8 bytes for an inner class, total usage rounded up to multiple of 8.}
	\def\enotesize{\normalsize}
	\theendnotes
\end{document}